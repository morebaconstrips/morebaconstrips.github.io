<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="description" content="Jan 2023 - Aug 2023
The project involved conducting in-depth research based on Carlini and Farid&amp;rsquo;s paper into potential white and black box attacks on ResNet50, aiming to significantly reduce the classifier&amp;rsquo;s accuracy to near 0%. Subsequently, one of the proposed white-box attacks (a Loss-Maximizing-like attack) was implemented to attack the ResNet50 model fine-tuned with the TrueFace dataset provided by the University of Trento.
The ultimate goal of the project was to enable ResNet50 to misclassify all the fake images extracted from the database and to make it falsely identify these images as real." />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://morebaconstrips.github.io/projects/adversarial/" />


    <title>
        
            Adversarial Examples Evading deepfake-image detectors with white-box attacks :: Gabriele Abbate  â€” Cybersecurity MSc student
        
    </title>





<link rel="stylesheet" href="/main.b78c3be9451dc4ca61ca377f3dc2cf2e6345a44c2bae46216a322ef366daa399.css" integrity="sha256-t4w76UUdxMphyjd/PcLPLmNFpEwrrkYhajIu82bao5k=">


    
        <link rel="stylesheet" type="text/css" href="custom.css">
    


    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
    <link rel="shortcut icon" href="/favicon.ico">
    <meta name="msapplication-TileColor" content="">


<meta itemprop="name" content="Adversarial Examples Evading deepfake-image detectors with white-box attacks">
<meta itemprop="description" content="Jan 2023 - Aug 2023
The project involved conducting in-depth research based on Carlini and Farid&rsquo;s paper into potential white and black box attacks on ResNet50, aiming to significantly reduce the classifier&rsquo;s accuracy to near 0%. Subsequently, one of the proposed white-box attacks (a Loss-Maximizing-like attack) was implemented to attack the ResNet50 model fine-tuned with the TrueFace dataset provided by the University of Trento.
The ultimate goal of the project was to enable ResNet50 to misclassify all the fake images extracted from the database and to make it falsely identify these images as real."><meta itemprop="datePublished" content="2023-09-16T19:01:48+02:00" />
<meta itemprop="dateModified" content="2023-09-16T19:01:48+02:00" />
<meta itemprop="wordCount" content="199"><meta itemprop="image" content="https://morebaconstrips.github.io/" />
<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://morebaconstrips.github.io/" /><meta name="twitter:title" content="Adversarial Examples Evading deepfake-image detectors with white-box attacks"/>
<meta name="twitter:description" content="Jan 2023 - Aug 2023
The project involved conducting in-depth research based on Carlini and Farid&rsquo;s paper into potential white and black box attacks on ResNet50, aiming to significantly reduce the classifier&rsquo;s accuracy to near 0%. Subsequently, one of the proposed white-box attacks (a Loss-Maximizing-like attack) was implemented to attack the ResNet50 model fine-tuned with the TrueFace dataset provided by the University of Trento.
The ultimate goal of the project was to enable ResNet50 to misclassify all the fake images extracted from the database and to make it falsely identify these images as real."/>



    <meta property="og:title" content="Adversarial Examples Evading deepfake-image detectors with white-box attacks" />
<meta property="og:description" content="Jan 2023 - Aug 2023
The project involved conducting in-depth research based on Carlini and Farid&rsquo;s paper into potential white and black box attacks on ResNet50, aiming to significantly reduce the classifier&rsquo;s accuracy to near 0%. Subsequently, one of the proposed white-box attacks (a Loss-Maximizing-like attack) was implemented to attack the ResNet50 model fine-tuned with the TrueFace dataset provided by the University of Trento.
The ultimate goal of the project was to enable ResNet50 to misclassify all the fake images extracted from the database and to make it falsely identify these images as real." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://morebaconstrips.github.io/projects/adversarial/" /><meta property="og:image" content="https://morebaconstrips.github.io/" /><meta property="article:section" content="projects" />
<meta property="article:published_time" content="2023-09-16T19:01:48+02:00" />
<meta property="article:modified_time" content="2023-09-16T19:01:48+02:00" />







    <meta property="article:published_time" content="2023-09-16 19:01:48 &#43;0200 CEST" />












    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text ">
                gabriele.portfolio.sh</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="/about">~/about</a></li><li><a href="/projects">~/projects</a></li><li><a href="/writeups">~/writeups</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            
            </p>
        </div>

        <article>
            <h2 class="post-title"><a href="https://morebaconstrips.github.io/projects/adversarial/">Adversarial Examples Evading deepfake-image detectors with white-box attacks</a></h2>

            
            
            

            <div class="post-content">
                <p>Jan 2023 - Aug 2023</p>
<hr>
<p>The project involved conducting in-depth research based on Carlini and Farid&rsquo;s <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w39/Carlini_Evading_Deepfake-Image_Detectors_With_White-_and_Black-Box_Attacks_CVPRW_2020_paper.pdf">paper</a> into potential white and black box attacks on ResNet50, aiming to significantly reduce the classifier&rsquo;s accuracy to near 0%. Subsequently, one of the proposed white-box attacks (a Loss-Maximizing-like attack) was implemented to attack the ResNet50 model fine-tuned with the TrueFace dataset provided by the University of Trento.</p>
<p>The ultimate goal of the project was to enable ResNet50 to misclassify all the fake images extracted from the database and to make it falsely identify these images as real. This ambitious objective aimed to demonstrate the susceptibility of the classifier to adversarial attacks, highlight potential vulnerabilities in image recognition systems, and propose some ways to prevent these attacks to happen.</p>
<p>The results were remarkable, as we successfully lowered the neural network&rsquo;s accuracy from 100% to 0%, even with small perturbations, as can be seen in the plots below.</p>
<p><img src="/images/adversarial/ROC.png" alt="display">
<img src="/images/adversarial/accuracy.png" alt="display"></p>
<p>N.B. Due to safety and ethical considerations, the code developed for this project has not been published. This decision was made to prevent any unintended or malicious use of the attack strategy and to mitigate the potential risks associated with deploying such techniques in real-world scenarios</p>

            </div>
        </article>

        <hr />

        <div class="post-info">
            
            
  		</div>
    </main>

            </div>

            
                <footer class="footer">
    
    
</footer>

            
        </div>

        



<script type="text/javascript" src="/bundle.min.46e438bd2eca7eb6358c700b40f6f56b22ab0b92503ecc11a2d8e4bcc610b8b4da83e5ea3d3e84fdb5f3c3c765744b9f5bbaf43cf5a027910be07ee39a9c12a1.js" integrity="sha512-RuQ4vS7KfrY1jHALQPb1ayKrC5JQPswRotjkvMYQuLTag&#43;XqPT6E/bXzw8dldEufW7r0PPWgJ5EL4H7jmpwSoQ=="></script>



    </body>
</html>
